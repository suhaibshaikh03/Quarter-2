{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTQfEMvKx-cc",
        "outputId": "03e11a3a-6810-4ace-ae49-a5c63f005449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain_community langchain_core langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "kF4ieaF-yZIn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "rNn-WX5_zqpv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm : ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.7  # Adjust for creativity\n",
        ")"
      ],
      "metadata": {
        "id": "0hjDUvS5ybXf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate.from_template(\"You are a helpful assistant. Answer the following question:\\n\\n {question}\")"
      ],
      "metadata": {
        "id": "zrstwgBr0FTr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just another helpter function\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def to_markdown(text)-> Markdown:\n",
        "    text : str = text.replace(\"•\", \"  *\")\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "sKt0JYiC4rRM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chain with a sample question\n",
        "question = \"What is LangChain?\"\n",
        "# below code for dynamic questioning at runtime from the user\n",
        "# question = input(\"How may I help you?\")\n",
        "a = prompt_template.invoke({\"question\": question})\n",
        "response : AIMessage = llm.invoke([HumanMessage(content = a.text, name=\"Suhaib\")])\n",
        "display(to_markdown(response.content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "y5spaklB1WuU",
        "outputId": "5a1c8e43-482d-48a7-e97a-ea1f139fed1f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> LangChain is a framework designed to simplify the development of applications using large language models (LLMs). Think of it as a toolkit that provides building blocks and utilities to help you create more sophisticated and powerful applications powered by AI.\n> \n> Here's a breakdown of what LangChain offers and what it's useful for:\n> \n> **Key Features and Concepts:**\n> \n> * **Chains:** LangChain allows you to create \"chains\" of actions. This means you can link together multiple LLM calls or other operations to achieve a more complex goal. For example, you could chain together an LLM call to summarize a document, followed by another LLM call to answer questions based on that summary.\n> * **Components:** LangChain provides a variety of reusable components, such as:\n>     * **Model I/O (Input/Output):**  Tools for interacting with different LLMs (like OpenAI, Hugging Face models, etc.) and handling their inputs and outputs.\n>     * **Prompt Templates:**  Helps you structure and manage prompts for LLMs, making it easier to experiment and reuse them.\n>     * **Indexes:**  Tools for indexing and retrieving data, which is crucial for LLMs to access relevant information.\n>     * **Memory:** Mechanisms for storing and retrieving information from past interactions, enabling conversational applications.\n>     * **Agents:**  Allows LLMs to decide which actions to take based on the current context, enabling more dynamic and autonomous behavior.\n>     * **Callbacks:**  A system for observing and customizing the execution of chains, useful for logging, debugging, and monitoring.\n> * **Abstraction:** LangChain provides an abstraction layer over different LLMs and other tools, making it easier to switch between models and experiment with different configurations. You don't need to worry about the specific API details of each model.\n> * **Modularity:** LangChain is designed to be modular, so you can pick and choose the components you need for your specific application.\n> * **Extensibility:** It's relatively easy to add custom components and integrations to LangChain.\n> \n> **What can you use LangChain for?**\n> \n> LangChain is useful for building a wide range of LLM-powered applications, including:\n> \n> * **Chatbots and Conversational Agents:** Creating more interactive and context-aware conversational experiences.\n> * **Question Answering Systems:** Building applications that can answer questions based on specific documents or knowledge bases.\n> * **Text Summarization and Generation:** Automating tasks like summarizing articles or generating creative content.\n> * **Data Analysis and Extraction:**  Extracting specific information from unstructured text data.\n> * **Automated Workflows:**  Creating complex workflows that use LLMs to automate tasks.\n> * **Personalized Experiences:** Building applications that adapt to individual user needs and preferences.\n> * **Research and Development:**  Experimenting with different LLMs and techniques to explore the possibilities of AI.\n> \n> **In Summary:**\n> \n> LangChain is a powerful and versatile framework that helps developers build sophisticated applications by leveraging the power of large language models. It provides the tools, components, and abstractions needed to chain together different LLM calls, manage prompts, access data, and create more dynamic and intelligent applications. It's a valuable resource for anyone looking to explore and build with the latest advances in AI.\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}